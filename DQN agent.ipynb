{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27253,"status":"ok","timestamp":1668178517765,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"kUlxMM-C-h2l","outputId":"59dd9a48-664d-4190-d0a8-e799f132ad12"},"outputs":[],"source":["import os\n","from google.colab import drive\n","MOUNTPOINT = '/content/gdrive'\n","drive.mount(MOUNTPOINT, force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1668178517766,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"mwF9_kfJA9CZ"},"outputs":[],"source":["path_to_folder = '/content/gdrive/Shareddrives/RL_MiniHack'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42546,"status":"ok","timestamp":1668178560307,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"ltiQ4-A7yK0A","outputId":"5d33243e-56c3-4966-d4b4-553952683133"},"outputs":[],"source":["!sudo apt update\n","!sudo apt install -y build-essential autoconf libtool pkg-config python3-dev \\\n","    python3-pip python3-numpy git flex bison libbz2-dev\n","\n","!wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | sudo apt-key add -\n","!sudo apt-add-repository 'deb https://apt.kitware.com/ubuntu/ bionic main'\n","!sudo apt-get update && apt-get --allow-unauthenticated install -y \\\n","    cmake \\\n","    kitware-archive-keyring\n","\n","# feel free to use a more elegant solution to make /usr/bin/cmake the default one\n","!sudo rm $(which cmake)\n","!$(which cmake) --version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92203,"status":"ok","timestamp":1668178652501,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"sKh2uSpfyDOa","outputId":"3ebd9e3c-2bcb-47e5-daef-28d7f6f33983"},"outputs":[],"source":["!pip3 install -Uv nle"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17327,"status":"ok","timestamp":1668178669824,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"jTkWNUWw2Qxs","outputId":"888814b7-5b95-4610-c5bc-e5b25a15a6f9"},"outputs":[],"source":["!pip install minihack"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8424,"status":"ok","timestamp":1668178678238,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"B5SfbJJfNCJl","outputId":"73241068-69c4-40e2-cc62-dce311f8da31"},"outputs":[],"source":["!pip install imageio_ffmpeg"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1688,"status":"ok","timestamp":1668178679917,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"HFK_Hmd4yh_T"},"outputs":[],"source":["import nle, gym\n","from gym import spaces\n","from nle import nethack\n","import minihack\n","from minihack import RewardManager\n","\n","import math\n","import os\n","import random\n","import operator\n","from datetime import datetime\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import flatten\n","from torch.nn import BatchNorm2d, Conv2d, CrossEntropyLoss, Dropout, Linear, MaxPool2d, Module, ReLU, Sequential, Softmax\n","\n","from gym.wrappers.monitoring import video_recorder\n","from IPython.display import HTML\n","from IPython import display \n","import glob\n","import matplotlib.pyplot as plt\n","\n","import cv2\n","import moviepy\n","import moviepy.video.io.ImageSequenceClip"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1668178679918,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"UoSUw-jpC-w6"},"outputs":[],"source":["savedir = '/content/gdrive/Shareddrives/RL_MiniHack/save_dir1'"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1668178679918,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"Cy-R5yHdJN8K"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1668178679919,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"_gPdd5PPOCUX"},"outputs":[],"source":["def plot_results(env_name, scores, color, ylim):\n","    \"\"\"Plots the reward attained by an Agent at each step of training in \n","        an environment for each iteration and average over all iterations\"\"\"\n","    \n","    plt.figure(figsize=(12,6))\n","    \n","    # Plot individual iterations \n","    for score in scores:\n","        plt.plot(score, alpha =0.1, color=color)\n","    \n","    # Plot mean over all iterations\n","    mean = np.mean(scores,axis=0)\n","    plt.plot(mean, color=color,label=\"Mean Reward\")\n","    \n","    plt.title(f\"DQN with PER - {env_name}\")\n","    plt.xlabel(\"Episode Number\")\n","    plt.ylabel(\"Reward\")\n","    plt.yticks(np.arange(ylim[0], ylim[1], 1.00))\n","    plt.legend(loc=4)\n","    plt.savefig(f\"DQN with PER-{env_name}.pdf\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1668178679919,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"9gw8NzCmJ1Q_"},"outputs":[],"source":["def format_state(state):\n","    \"\"\"Formats state into form that the NN can accept\"\"\"\n","    glyphs = state[\"glyphs\"]\n","    # Normalize\n","    glyphs = glyphs/glyphs.max()\n","    glyphs = glyphs.reshape((1,1,21,79))\n","    return torch.from_numpy(glyphs).squeeze(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1668178679919,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"yxtWROtvMj2O"},"outputs":[],"source":["def moving_average(arr, win_size):\n","    sum = np.cumsum(arr, dtype=float)\n","    sum[win_size:] = sum[win_size:] - sum[:-win_size]\n","    return sum/win_size"]},{"cell_type":"markdown","metadata":{"id":"MJRDxrfCdhs4"},"source":["### DQN Model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1668178679919,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"pkNWPUGmqWee"},"outputs":[],"source":["class SegmentTree(object):\n","    def __init__(self, capacity, operation, neutral_element):\n","        \"\"\"Build a Segment Tree data structure.\n","        https://en.wikipedia.org/wiki/Segment_tree\n","        Can be used as regular array, but with two\n","        important differences:\n","            a) setting item's value is slightly slower.\n","               It is O(lg capacity) instead of O(1).\n","            b) user has access to an efficient ( O(log segment size) )\n","               `reduce` operation which reduces `operation` over\n","               a contiguous subsequence of items in the array.\n","        Paramters\n","        ---------\n","        capacity: int\n","            Total size of the array - must be a power of two.\n","        operation: lambda obj, obj -> obj\n","            and operation for combining elements (eg. sum, max)\n","            must form a mathematical group together with the set of\n","            possible values for array elements (i.e. be associative)\n","        neutral_element: obj\n","            neutral element for the operation above. eg. float('-inf')\n","            for max and 0 for sum.\n","        \"\"\"\n","        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n","        self._capacity = capacity\n","        self._value = [neutral_element for _ in range(2 * capacity)]\n","        self._operation = operation\n","\n","    def _reduce_helper(self, start, end, node, node_start, node_end):\n","        if start == node_start and end == node_end:\n","            return self._value[node]\n","        mid = (node_start + node_end) // 2\n","        if end <= mid:\n","            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n","        else:\n","            if mid + 1 <= start:\n","                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n","            else:\n","                return self._operation(\n","                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n","                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n","                )\n","\n","    def reduce(self, start=0, end=None):\n","        \"\"\"Returns result of applying `self.operation`\n","        to a contiguous subsequence of the array.\n","            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n","        Parameters\n","        ----------\n","        start: int\n","            beginning of the subsequence\n","        end: int\n","            end of the subsequences\n","        Returns\n","        -------\n","        reduced: obj\n","            result of reducing self.operation over the specified range of array elements.\n","        \"\"\"\n","        if end is None:\n","            end = self._capacity\n","        if end < 0:\n","            end += self._capacity\n","        end -= 1\n","        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n","\n","    def __setitem__(self, idx, val):\n","        # index of the leaf\n","        idx += self._capacity\n","        self._value[idx] = val\n","        idx //= 2\n","        while idx >= 1:\n","            self._value[idx] = self._operation(\n","                self._value[2 * idx],\n","                self._value[2 * idx + 1]\n","            )\n","            idx //= 2\n","\n","    def __getitem__(self, idx):\n","        assert 0 <= idx < self._capacity\n","        return self._value[self._capacity + idx]\n","\n","\n","class SumSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(SumSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=operator.add,\n","            neutral_element=0.0\n","        )\n","\n","    def sum(self, start=0, end=None):\n","        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n","        return super(SumSegmentTree, self).reduce(start, end)\n","\n","    def find_prefixsum_idx(self, prefixsum):\n","        \"\"\"Find the highest index `i` in the array such that\n","            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n","        if array values are probabilities, this function\n","        allows to sample indexes according to the discrete\n","        probability efficiently.\n","        Parameters\n","        ----------\n","        perfixsum: float\n","            upperbound on the sum of array prefix\n","        Returns\n","        -------\n","        idx: int\n","            highest index satisfying the prefixsum constraint\n","        \"\"\"\n","        try:\n","            assert 0 <= prefixsum <= self.sum() + 1e-5\n","        except AssertionError:\n","            print(\"Prefix sum error: {}\".format(prefixsum))\n","            exit()\n","        idx = 1\n","        while idx < self._capacity:  # while non-leaf\n","            if self._value[2 * idx] > prefixsum:\n","                idx = 2 * idx\n","            else:\n","                prefixsum -= self._value[2 * idx]\n","                idx = 2 * idx + 1\n","        return idx - self._capacity\n","\n","\n","class MinSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(MinSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=min,\n","            neutral_element=float('inf')\n","        )\n","\n","    def min(self, start=0, end=None):\n","        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n","\n","        return super(MinSegmentTree, self).reduce(start, end)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1668178679919,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"yWJ_1N89Kxdv"},"outputs":[],"source":["#Modified for Prioritized Experience Replay\n","class ReplayBuffer:\n","    \"\"\"\n","    Simple storage for transitions from an environment.\n","    \"\"\"\n","\n","    def __init__(self, size, alpha=0.6, beta_start=0.4, beta_frames=100000):\n","        \"\"\"\n","        Initialise a buffer of a given size for storing transitions\n","        :param size: the maximum number of transitions that can be stored\n","        \"\"\"\n","        self._storage = []\n","        self._maxsize = size\n","        self._next_idx = 0\n","\n","        assert alpha >= 0\n","        self._alpha = alpha\n","\n","        self.beta_start = beta_start\n","        self.beta_frames = beta_frames\n","        self.frame=1\n","\n","        it_capacity = 1\n","        while it_capacity < size:\n","            it_capacity *= 2\n","\n","        self._it_sum = SumSegmentTree(it_capacity)\n","        self._it_min = MinSegmentTree(it_capacity)\n","        self._max_priority = 1.0\n","\n","    def __len__(self):\n","        return len(self._storage)\n","\n","    def add(self, state, action, reward, next_state, done):\n","        \"\"\"\n","        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n","        :param state: the agent's initial state\n","        :param action: the action taken by the agent\n","        :param reward: the reward the agent received\n","        :param next_state: the subsequent state\n","        :param done: whether the episode terminated\n","        \"\"\"\n","        data = (state, action, reward, next_state, done)\n","        idx = self._next_idx\n","        if self._next_idx >= len(self._storage):\n","            self._storage.append(data)\n","        else:\n","            self._storage[self._next_idx] = data\n","        self._next_idx = (self._next_idx + 1) % self._maxsize\n","\n","        self._it_sum[idx] = self._max_priority ** self._alpha\n","        self._it_min[idx] = self._max_priority ** self._alpha\n","    \n","    def beta_by_frame(self, frame_idx):\n","        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n","    \n","    def _sample_proportional(self, batch_size):\n","        res = []\n","        for _ in range(batch_size):\n","            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)\n","            idx = self._it_sum.find_prefixsum_idx(mass)\n","            res.append(idx)\n","        return res\n","\n","    def _encode_sample(self, indices):\n","        states, actions, rewards, next_states, dones = [], [], [], [], []\n","        for i in indices:\n","            data = self._storage[i]\n","            state, action, reward, next_state, done = data\n","            states.append(np.array(state, copy=False))\n","            actions.append(action)\n","            rewards.append(reward)\n","            next_states.append(np.array(next_state, copy=False))\n","            dones.append(done)\n","        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n","\n","    def sample(self, batch_size):\n","        \"\"\"\n","        Randomly sample a batch of transitions from the buffer.\n","        :param batch_size: the number of transitions to sample\n","        :return: a mini-batch of sampled transitions\n","        \"\"\"\n","        idxes = self._sample_proportional(batch_size)\n","\n","        weights = []\n","\n","        #find smallest sampling prob: p_min = smallest priority^alpha / sum of priorities^alpha\n","        p_min = self._it_min.min() / self._it_sum.sum()\n","\n","        beta = self.beta_by_frame(self.frame)\n","        self.frame+=1\n","        \n","        #max_weight given to smallest prob\n","        max_weight = (p_min * len(self._storage)) ** (-beta)\n","\n","        for idx in idxes:\n","            p_sample = self._it_sum[idx] / self._it_sum.sum()\n","            weight = (p_sample * len(self._storage)) ** (-beta)\n","            weights.append(weight / max_weight)\n","        weights = torch.tensor(weights, device=device, dtype=torch.float) \n","        encoded_sample = self._encode_sample(idxes)\n","        return encoded_sample, idxes, weights\n","\n","        '''indices = np.random.randint(0, len(self._storage) - 1, size=batch_size)\n","        return self._encode_sample(indices)'''\n","\n","    def update_priorities(self, idxes, priorities):\n","        \"\"\"Update priorities of sampled transitions.\n","        sets priority of transition at index idxes[i] in buffer\n","        to priorities[i].\n","        Parameters\n","        ----------\n","        idxes: [int]\n","            List of idxes of sampled transitions\n","        priorities: [float]\n","            List of updated priorities corresponding to\n","            transitions at the sampled idxes denoted by\n","            variable `idxes`.\n","        \"\"\"\n","        assert len(idxes) == len(priorities)\n","        for idx, priority in zip(idxes, priorities):\n","            assert 0 <= idx < len(self._storage)\n","            self._it_sum[idx] = (priority+1e-5) ** self._alpha\n","            self._it_min[idx] = (priority+1e-5) ** self._alpha\n","\n","            self._max_priority = max(self._max_priority, (priority+1e-5))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1668178679920,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"bzOJqNzPb98D"},"outputs":[],"source":["class DQN(nn.Module):\n","    def __init__(self, input_shape, num_actions):\n","        super(DQN, self).__init__()\n","\n","        self.tanh = nn.Tanh()\n","\n","        self.conv1 = Conv2d(in_channels=1, out_channels=16, kernel_size=4, stride=1)\n","        self.conv2 = Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2)\n","        self.conv3 = Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2)\n","\n","        self.fc1 = Linear(in_features=1728, out_features=100)\n","        self.fc2 = Linear(in_features=100, out_features=num_actions)\n","        self.s1 = Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Returns the values of a forward pass of the network\n","        :param x: The input to feed into the network \n","        \"\"\"\n","        x = self.tanh(self.conv1(x))\n","        x = self.tanh(self.conv2(x))\n","        x = self.tanh(self.conv3(x))\n","\n","        # Define fully connected layers\n","        x = x.reshape(x.shape[0], -1)\n","        x = self.tanh(self.fc1(x))\n","        x = self.fc2(x)\n","        x = self.s1(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1668178790739,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"M0dR0X00b98D"},"outputs":[],"source":["class DQNAgent():\n","    def __init__(self, observation_space, action_space, **kwargs):\n","\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.action_space = action_space\n","        self.use_double_dqn = kwargs.get(\"use_double_dqn\", None)\n","        self.gamma = kwargs.get(\"gamma\", 0.99)\n","        self.lr = kwargs.get(\"lr\", None)\n","        self.experience_replay_size = kwargs.get(\"replay_buffer_size\", None)\n","        self.batch_size = kwargs.get(\"batch_size\", None)\n","        self.num_feats = observation_space['glyphs'].shape\n","        self.num_actions = action_space.n\n","\n","        self.declare_networks()\n","            \n","        #self.model.load_state_dict(torch.load(\"/content/gdrive/Shareddrives/RL_MiniHack/DQN_Agents/model maze-ez 8nov.pth\"))\n","        #self.target_model.load_state_dict(torch.load(\"/content/gdrive/Shareddrives/RL_MiniHack/DQN_Agents/target_model maze-ez 8nov.pth\"))\n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n","        \n","        #move to correct device\n","        self.model = self.model.to(self.device)\n","        self.target_model.to(self.device)\n","\n","        self.update_count = 0\n","\n","        self.declare_memory()\n","\n","    def declare_networks(self):\n","        self.model = DQN(self.num_feats, self.num_actions)\n","        self.target_model = DQN(self.num_feats, self.num_actions)\n","\n","    def declare_memory(self):\n","        self.memory = ReplayBuffer(self.experience_replay_size)\n","\n","    def append_to_replay(self, s, a, r, s_):\n","        self.memory.add((s, a, r, s_))\n","\n","    def prep_minibatch(self):\n","        # random transition batch is taken from experience replay memory\n","        encoded_sample, indices, weights  = self.memory.sample(self.batch_size)\n","        states, actions, rewards, next_states, dones = encoded_sample\n","        \n","        states = np.array(states)\n","        next_states = np.array(next_states)\n","        states = torch.from_numpy(states).float().to(device)\n","        actions = torch.from_numpy(actions).long().to(device)\n","        rewards = torch.from_numpy(rewards).float().to(device)\n","        next_states = torch.from_numpy(next_states).float().to(device)\n","        dones = torch.from_numpy(dones).float().to(device)\n","\n","        shape = (-1,)+self.num_feats\n","        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, next_states)), device=self.device, dtype=torch.uint8)\n","        try: #sometimes all next states are false\n","            non_final_next_states = torch.tensor([s for s in next_states if s is not None], device=self.device, dtype=torch.float).view(shape)\n","            empty_next_state_values = False\n","        except:\n","            non_final_next_states = None\n","            empty_next_state_values = True\n","\n","        return states, actions, rewards, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights\n","\n","    def compute_loss(self, batch_vars): #faster\n","        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n","\n","        #estimate\n","        #self.model.sample_noise()\n","        current_q_values = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze()\n","        \n","        #target\n","        with torch.no_grad():\n","            max_next_q_values = torch.zeros(self.batch_size, device=self.device, dtype=torch.float).unsqueeze(dim=1)\n","            if not empty_next_state_values:\n","                max_next_action = self.get_max_next_state_action(non_final_next_states)\n","                #self.target_model.sample_noise()\n","                max_next_q_values[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action)\n","            expected_q_values = batch_reward + ((self.gamma)*max_next_q_values.squeeze())\n","\n","        diff = (expected_q_values - current_q_values)\n","        self.memory.update_priorities(indices, diff.detach().squeeze().abs().cpu().numpy().tolist())\n","        loss = self.MSE(diff).squeeze() * weights\n","        loss = loss.mean()\n","\n","        return loss\n","\n","    def update(self, s, a, r, s_):\n","        batch_vars = self.prep_minibatch()\n","        loss = self.compute_loss(batch_vars)\n","\n","        # Optimize the model\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        for param in self.model.parameters():\n","            param.grad.data.clamp_(-1, 1)\n","        self.optimizer.step()\n","\n","        return loss\n","\n","    def act(self, observation):\n","        if not torch.cuda.is_available():\n","            observation = observation.type(torch.FloatTensor) \n","        else:\n","            observation = observation.type(torch.cuda.FloatTensor) \n","        state = torch.unsqueeze(observation, 0).to(device)\n","        result = self.model.forward(state)\n","        action = torch.argmax(result).item()\n","        #print(result)\n","        return action\n","\n","    def update_target_model(self):\n","        self.target_model.load_state_dict(self.model.state_dict())\n","\n","    def get_max_next_state_action(self, next_states):\n","        return self.target_model(next_states).max(dim=1)\n","    \n","    def MSE(self, x):\n","        return 0.5 * x.pow(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1668178750948,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"aJ2IcuOpb98D"},"outputs":[],"source":["def dqn(env, seed, learning_rate, max_episodes, max_episode_length, gamma, verbose=True):\n","    \"\"\"\n","    Method to train DQN model.\n","    \n","    Input:\n","    env: The environment to be used during training\n","    seed: The random seed for any random operations performed \n","    learning_rate: The learning rate uesd for the Adam optimizer when training the model \n","    number_episodes: Number of episodes to train for \n","    max_episode_length: The maximum number of steps to take in an episode before terminating\n","    gamma: The discount factor used when calculating the discounted rewards of an episode\n","    verbose: Print episode reward after each episode\n","    \n","    Returns:\n","    scores: The cumulative reward achieved by the agent for each episode during traiing\n","    \"\"\"\n","\n","    hyper_params = {\n","        'replay-buffer-size': int(1e6),\n","        'learning-rate': 0.001,\n","        'gamma': gamma,  # discount factor\n","        'num-steps': int(7e5),  # Steps to run for, max episodes should be hit before this\n","        'batch-size': 32,  \n","        'learning-starts': 1000,  # set learning to start after 1000 steps of exploration\n","        'learning-freq': 1,  # Optimize after each step\n","        'use-double-dqn': False,\n","        'target-update-freq': 1000, # number of iterations between every target network update\n","        'eps-start': 1.0,  # e-greedy start threshold \n","        'eps-end': 0.1,  # e-greedy end threshold \n","        'eps-fraction': 0.6,  # Percentage of the time that epsilon is annealed\n","        'print-freq': 10,\n","\n","    }\n","    \n","    np.random.seed(seed)\n","    env.seed(seed)\n","    \n","    # Create DQN agent\n","\n","    agent = DQNAgent(\n","        env.observation_space, \n","        env.action_space,\n","        train=True,\n","        use_double_dqn=hyper_params['use-double-dqn'],\n","        lr=hyper_params['learning-rate'],\n","        batch_size=hyper_params['batch-size'],\n","        gamma=hyper_params['gamma'],\n","        replay_buffer_size=hyper_params['replay-buffer-size']\n","    )\n","    \n","    # define variables to track agent metrics\n","    total_reward = 0\n","    scores = []\n","    mean_rewards = []\n","\n","    # Reset gym env before training\n","    state = format_state(env.reset())\n","    eps_timesteps = hyper_params['eps-fraction'] * float(hyper_params['num-steps'])\n","\n","    actions_taken = []\n","    #video = []\n","\n","    # Train for set number of steps\n","    for t in range(hyper_params['num-steps']):\n","\n","        # determine exploration probability\n","        fract = min(1.0, float(t) / eps_timesteps)\n","        eps_threshold = hyper_params[\"eps-start\"] + fract * (hyper_params[\"eps-end\"] - hyper_params[\"eps-start\"])\n","        sample = random.random()\n","\n","        # Decide to explore and choose random action or use model to act\n","        if sample < eps_threshold:\n","            action = np.random.choice(agent.action_space.n)\n","        else:\n","            action = agent.act(state)\n","            actions_taken.append(action)\n","\n","        # Take step in environment\n","        next_state, reward, done, _ = env.step(action)\n","        #video.append(next_state['pixel'])\n","        next_state = format_state(next_state)\n","        agent.memory.add(state, action, reward, next_state, float(done))\n","        total_reward += reward\n","        state = next_state\n","        \n","        if done:\n","            scores.append(total_reward)\n","            print(f\"episode reward: {total_reward} \", f\"{len(actions_taken)} \" 'actions taken: {}'.format(actions_taken))\n","            np.random.seed(seed)\n","            env.seed(seed)\n","            state = format_state(env.reset())\n","            '''if t % 10000 == 0:\n","              clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(video, fps=4)\n","              clip.write_videofile(\"/content/gdrive/Shareddrives/RL_MiniHack/DQN_Agents/videos/jared/{}.mp4\".format(\"qeust-h \"+str(num_episodes)))\n","            video.clear()'''\n","            total_reward = 0\n","            actions_taken.clear()\n","            \n","\n","        if t > hyper_params['learning-starts'] and t % hyper_params['learning-freq'] == 0:\n","            td_loss = agent.update(state, action, reward, next_state)\n","\n","        if t > hyper_params['learning-starts'] and t % hyper_params['target-update-freq'] == 0:\n","            td_loss = agent.update_target_model()\n","\n","        num_episodes = len(scores)\n","        if done and hyper_params['print-freq'] is not None and len(scores) % hyper_params['print-freq'] == 0:\n","            mean_100ep_reward = round(np.mean(scores[-101:-1]), 1)\n","            mean_rewards.append(mean_100ep_reward)\n","            print('********************************************************')\n","            print('steps: {}'.format(t))\n","            print('episodes: {}'.format(num_episodes))\n","            print('mean 100 episode reward: {}'.format(mean_100ep_reward))\n","            print('% time spent exploring: {}'.format(eps_threshold))\n","            print('********************************************************')\n","\n","   \n","  \n","        '''if num_episodes >= max_episodes:\n","            return agent, scores'''\n","\n","    return agent, scores"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1668178679920,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"k0qRw4ecb98E"},"outputs":[],"source":["def run_dqn(env, number_episodes, max_episode_length, iterations, model_file_name):\n","    \"\"\"Trains DQN model for a number of episodes on a given environment\"\"\"\n","    seeds = np.random.randint(1000, size=iterations)\n","    scores_arr = [] \n","    \n","    for seed in seeds:\n","        print(\"\\nseed: \", seed)\n","\n","        # Train the DQN Model \n","        agent, scores = dqn(env=env, \n","                            seed=seed, \n","                            learning_rate=0.01,\n","                            max_episodes=number_episodes, \n","                            max_episode_length=max_episode_length, \n","                            gamma=0.99 ,\n","                            verbose=True)\n","        \n","        # Store rewards for this iteration\n","        torch.save(agent.model.state_dict(), \"/content/gdrive/Shareddrives/RL_MiniHack/DQN_Agents/model {}.pth\".format(model_file_name))\n","        torch.save(agent.target_model.state_dict(), \"/content/gdrive/Shareddrives/RL_MiniHack/DQN_Agents/target_model {}.pth\".format(model_file_name))\n","        scores_arr.append(scores)\n","        \n","    return agent, scores_arr"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1668178679921,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"6zMbHUl_gvIW"},"outputs":[],"source":["def save_video_of_model(agent, env, model_file_name):\n","    state = (env.reset())\n","    video = []\n","    done = False\n","    i = 0\n","    today = datetime.now()\n","    curr_datetime = str(today.date()) + \" \" + str(today.hour) + \"_\" + str(today.minute)\n","    \n","    # continues until completion\n","    while not done:\n","        video.append(state['pixel'])\n","        state = format_state(state)\n","        action = agent.act(state)\n","        print(action)\n","        state, reward, done, _ = env.step(action)\n","    fps=4\n","\n","    clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(video, fps=fps)\n","    clip.write_videofile(\"/content/gdrive/Shareddrives/RL_MiniHack/DQN_Agents/videos{}.mp4\".format(model_file_name+curr_datetime))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1668178679921,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"gmQdTLKPcLRY"},"outputs":[],"source":["def maze_explore_reward(env, prev_obs, action, next_obs):\n","    if (prev_obs[0] == 2359).sum() > (next_obs[0] == 2359).sum():\n","        return 0.1\n","    return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1668178679921,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"BHxlV1jxcMBT"},"outputs":[],"source":["reward_gen = RewardManager()\n","\n","# Random reward included to prevent reward glitch \n","reward_gen.add_eat_event(\"apple\", reward=0.5)\n","\n","# Custom Rewards for long corridors at top and bottom \n","reward_gen.add_coordinate_event((3,27), reward = -5, terminal_required = False)\n","reward_gen.add_coordinate_event((3,28), reward = -5, terminal_required = False)\n","reward_gen.add_coordinate_event((3,29), reward = -5, terminal_required = False)\n","\n","reward_gen.add_coordinate_event((19,27), reward = -5, terminal_required = False)\n","reward_gen.add_coordinate_event((19,28), reward = -5, terminal_required = False)\n","reward_gen.add_coordinate_event((19,29), reward = -5, terminal_required = False)\n","\n","reward_gen.add_custom_reward_fn(maze_explore_reward)\n","\n","reward_gen.add_coordinate_event((11,27), reward = 100, terminal_required = False)# first door at end of maze\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1668178679921,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"uX8B47gecQT6"},"outputs":[],"source":["NAVIGATE_ACTIONS = (nethack.CompassDirection.N,\n","    nethack.CompassDirection.S,\n","    nethack.CompassDirection.W,\n","    nethack.CompassDirection.E,\n","    nethack.Command.EAT,\n","    nethack.Command.PICKUP)\n","#nethack.MiscDirection.DOWN)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1668178679921,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"MnsEbaiNcR3b"},"outputs":[],"source":["QUEST_ACTIONS = (\n","    nethack.CompassDirection.N,\n","    nethack.CompassDirection.E,\n","    nethack.CompassDirection.S,\n","    nethack.CompassDirection.W,\n","    nethack.CompassDirection.NW,\n","    nethack.CompassDirection.NE,\n","    nethack.CompassDirection.SW,\n","    nethack.CompassDirection.SE,\n","    nethack.Command.PICKUP,\n","    nethack.Command.EAT,\n","    nethack.Command.APPLY,\n","    nethack.Command.ZAP, \n","    nethack.Command.PUTON,\n","    nethack.Command.QUAFF,\n","    nethack.Command.WIELD,\n","    nethack.Command.RUSH,\n","    nethack.Command.OPEN)"]},{"cell_type":"markdown","metadata":{"id":"vyWnemnDrdSv"},"source":["### RUNS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9JVe33xRkSJQ"},"outputs":[],"source":["#,\"pixel\",\"message\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oO7Uq8mmrdSv"},"outputs":[],"source":["'''env_name = \"MiniHack-Quest-Easy-v0\"\n","env = gym.make(env_name, observation_keys=[\"glyphs\"], actions=QUEST_ACTIONS)\n","agent_easy, quest_easy_scores = run_dqn(env, number_episodes=1000, max_episode_length=1000, iterations=1, model_file_name='quest-ez 7nov')\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHsLyCqcjv1V","outputId":"63c1cac1-d454-4d13-b4b1-ff8b732dd380"},"outputs":[],"source":["env_name = \"MiniHack-Quest-Hard-v0\"\n","env = gym.make(env_name, observation_keys=[\"glyphs\"], actions=QUEST_ACTIONS)\n","print(env.observation_space['glyphs'].shape, env.action_space.n)\n","agent_hard, quest_hard_scores = run_dqn(env, number_episodes=1000, max_episode_length=1000, iterations=2, model_file_name='quest-h ')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9cDhZxEiU74"},"outputs":[],"source":["'''env_name = \"MiniHack-ExploreMaze-Easy-v0\"\n","env = gym.make(env_name, observation_keys=[\"glyphs\"], actions=QUEST_ACTIONS)\n","print(env.observation_space['glyphs'].shape, env.action_space.n)\n","agentMaze, maze_scores = run_dqn(env, number_episodes=1000, max_episode_length=1000, iterations=1, model_file_name=\"maze-ez 8nov\")\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"07cmHActnMlH"},"outputs":[],"source":["'''np.savetxt(\"/content/gdrive/Shareddrives/RL_MiniHack/DQN_Agents/quest_hard_j.txt\", quest_hard_scores)\n","torch.save(agent.online_network.state_dict(), \"/content/gdrive/Shareddrives/RL_MiniHack/DQN_Agents/online_model_j.pth\")\n","torch.save(agent.target_network.state_dict(), \"/content/gdrive/Shareddrives/RL_MiniHack/DQN_Agents/target_model_j.pth\")'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a0hx2SoosMJo"},"outputs":[],"source":["plot_results(env_name=env_name,scores=quest_hard_scores, ylim=(-12,8), color=\"red\" )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"elapsed":1130,"status":"ok","timestamp":1668177755136,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"qB9Rj_sO8rS4","outputId":"a2ca66c8-431e-4172-e30c-531f99bb1309"},"outputs":[],"source":["#plot_results(env_name=env_name,scores=maze_scores,ylim=(-12,8), color=\"red\" )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11790,"status":"ok","timestamp":1668177879905,"user":{"displayName":"Nico Govindsamy","userId":"15008135566748512591"},"user_tz":-120},"id":"r4gJrKp3oUYj","outputId":"4088079c-3df2-4f2b-91d9-1dc00c8145b7"},"outputs":[],"source":["env = gym.make(env_name, observation_keys=[\"glyphs\", \"pixel\"])\n","save_video_of_model(agent_hard, env, \"quest-h \")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[{"file_id":"1tQNTqIga5nxdXky5xMaK9XYWb5jksFdj","timestamp":1666432821333}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.7.9 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.9"},"vscode":{"interpreter":{"hash":"a2a8a77d7403fdb126924c18613820ef642b0f12c0ca1eaff1aa8ec7198d047c"}}},"nbformat":4,"nbformat_minor":0}
